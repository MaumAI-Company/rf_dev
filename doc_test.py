from ragflow_sdk import RAGFlow
import requests
import json
import re
import uuid
import os
from pathlib import Path
import time
import asyncio
import aiohttp
from typing import List, Dict

# RAGFlow HTTP API ì„¤ì •
api_key = "ragflow-U5ZGEyNTdlNjkyODExZjBiODE2MDI0Mm"
base_url = "http://10.50.7.154:8080"
plus_base_url = "http://10.50.7.154:5000"
headers = {
    "Authorization": f"Bearer {api_key}",
    "Content-Type": "application/json"
}

# íŒŒì‹± íƒ€ì„ì•„ì›ƒ ì„¤ì • (ì´ˆ)
PARSING_TIMEOUT = 1800  # 30ë¶„ (í•„ìš”ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥)

async def parse_document_async(session: aiohttp.ClientSession, doc_id: str, doc_name: str = None, 
                               timeout: int = PARSING_TIMEOUT) -> Dict:
    """
    ë¹„ë™ê¸°ë¡œ ë‹¨ì¼ ë¬¸ì„œ íŒŒì‹± ìš”ì²­ ë° ì™„ë£Œ ëŒ€ê¸°
    
    Args:
        session: aiohttp ì„¸ì…˜
        doc_id: ë¬¸ì„œ ID
        doc_name: ë¬¸ì„œ ì´ë¦„ (ë¡œê¹…ìš©)
        timeout: íŒŒì‹± íƒ€ì„ì•„ì›ƒ (ì´ˆ, ê¸°ë³¸ê°’: 1800ì´ˆ = 30ë¶„)
        
    Returns:
        dict: íŒŒì‹± ê²°ê³¼
    """
    url = f"{plus_base_url}/api/v1/knowledgebases/documents/{doc_id}/parse"
    display_name = doc_name or doc_id
    
    try:
        print(f"\nğŸ“¤ Sending parse request for: {display_name} (timeout: {timeout}s)")
        start_time = time.time()
        
        # íƒ€ì„ì•„ì›ƒ ì„¤ì •
        timeout_obj = aiohttp.ClientTimeout(total=timeout)
        
        # íŒŒì‹± ìš”ì²­ ë° ì‘ë‹µ ëŒ€ê¸° (ë¸”ë¡œí‚¹)
        async with session.post(url, headers=headers, timeout=timeout_obj) as response:
            result = await response.json()
            elapsed = int(time.time() - start_time)
            
            if result.get("code") == 0:
                print(f"âœ… Parsing completed for: {display_name} (Time: {elapsed}s)")
                return {
                    "doc_id": doc_id,
                    "name": display_name,
                    "success": True,
                    "result": result,
                    "elapsed": elapsed
                }
            else:
                print(f"âŒ Parsing failed for {display_name}: {result.get('message', 'Unknown error')}")
                return {
                    "doc_id": doc_id,
                    "name": display_name,
                    "success": False,
                    "result": result,
                    "elapsed": elapsed
                }
                
    except asyncio.TimeoutError:
        elapsed = int(time.time() - start_time)
        print(f"â±ï¸ Parsing timeout for {display_name} after {elapsed}s (limit: {timeout}s)")
        return {
            "doc_id": doc_id,
            "name": display_name,
            "success": False,
            "result": {"code": -1, "message": f"Timeout after {timeout}s"},
            "elapsed": elapsed
        }
    except Exception as e:
        elapsed = int(time.time() - start_time)
        print(f"âŒ Error parsing {display_name}: {e}")
        return {
            "doc_id": doc_id,
            "name": display_name,
            "success": False,
            "result": {"code": -1, "message": str(e)},
            "elapsed": elapsed
        }

async def parse_documents_sequentially(document_infos: List[Dict], timeout_per_doc: int = PARSING_TIMEOUT) -> Dict:
    """
    ë¬¸ì„œë¥¼ ìˆœì°¨ì ìœ¼ë¡œ íŒŒì‹± (í•œ ë¬¸ì„œ ì™„ë£Œ í›„ ë‹¤ìŒ ë¬¸ì„œ ì‹œì‘)
    
    Args:
        document_infos: ë¬¸ì„œ ì •ë³´ ë¦¬ìŠ¤íŠ¸ [{"doc_id": "...", "doc_name": "..."}, ...]
        timeout_per_doc: ë¬¸ì„œë‹¹ íŒŒì‹± íƒ€ì„ì•„ì›ƒ (ì´ˆ, ê¸°ë³¸ê°’: 1800ì´ˆ = 30ë¶„)
        
    Returns:
        dict: ë¬¸ì„œë³„ íŒŒì‹± ê²°ê³¼
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“„ Starting sequential document parsing ({len(document_infos)} documents)")
    print(f"   Timeout per document: {timeout_per_doc}s ({timeout_per_doc // 60}min)")
    print(f"{'='*60}")
    
    results = {}
    total_start_time = time.time()
    
    # ClientSessionì—ë„ ì „ì—­ íƒ€ì„ì•„ì›ƒ ì„¤ì •
    timeout_obj = aiohttp.ClientTimeout(total=None, connect=60, sock_read=timeout_per_doc)
    
    async with aiohttp.ClientSession(timeout=timeout_obj) as session:
        for idx, doc_info in enumerate(document_infos, 1):
            doc_id = doc_info["doc_id"]
            doc_name = doc_info.get("doc_name", doc_id)
            
            print(f"\n{'â”€'*60}")
            print(f"ğŸ“„ Document {idx}/{len(document_infos)}: {doc_name}")
            print(f"{'â”€'*60}")
            
            # íŒŒì‹± ìš”ì²­ ë° ì™„ë£Œ ëŒ€ê¸° (ìˆœì°¨ ì²˜ë¦¬)
            result = await parse_document_async(session, doc_id, doc_name, timeout=timeout_per_doc)
            results[doc_id] = result
    
    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    total_elapsed = int(time.time() - total_start_time)
    success_count = sum(1 for r in results.values() if r.get("success"))
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š Parsing Summary")
    print(f"{'='*60}")
    print(f"  Total documents: {len(document_infos)}")
    print(f"  Successful: {success_count}")
    print(f"  Failed: {len(document_infos) - success_count}")
    print(f"  Total time: {total_elapsed}s ({total_elapsed // 60}min)")
    if len(document_infos) > 0:
        print(f"  Average time per doc: {total_elapsed // len(document_infos)}s")
    print(f"{'='*60}\n")
    
    return results

def get_document_parsing_progress(doc_id: str) -> Dict:
    """
    ë™ê¸°ë¡œ ë¬¸ì„œ íŒŒì‹± ì§„í–‰ ìƒí™© í™•ì¸
    
    Args:
        doc_id: ë¬¸ì„œ ID
        
    Returns:
        dict: ë¬¸ì„œ ìƒíƒœ ì •ë³´
    """
    url = f"{plus_base_url}/api/v1/knowledgebases/documents/{doc_id}/parse/progress"

    try:
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code == 200:
            result = response.json()
            # resultì˜ ìƒ˜í”Œ: 
            # {'code': 0, 'data': {'message': 'íŒŒì‹± ì™„ë£Œ', 'progress': 1.0, 'running': '3', 'status': '1'}, 'message': 'ì‘ì—… ì„±ê³µ'}
            print(f"âœ… doc parse {doc_id}: {result}")
            return result
        else:
            print(f"âŒ Failed to get status for doc {doc_id}: HTTP {response.status_code}")
            return {"code": -1, "message": f"HTTP {response.status_code}"} 
    except Exception as e:
        print(f"âŒ Error getting status for doc {doc_id}: {e}")
        return {"code": -1, "message": str(e)}

    

# Ragflow SDK í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ doument ì—…ë¡œë“œ ë° íŒŒì‹±
def parse_test():
    """ë¬¸ì„œ ì—…ë¡œë“œ ë° íŒŒì‹± í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    rag_object = RAGFlow(api_key=api_key, base_url=f"{base_url}")
    
    # dataset ì´ë¦„
    dataset_name = "test"
    dataset = None
    
    try:
        # ê¸°ì¡´ dataset ê²€ìƒ‰ ì‹œë„
        datasets = rag_object.list_datasets(name=dataset_name)
        if len(datasets) > 0:
            dataset = datasets[0]
            print(f"Found existing dataset: {dataset.name} (ID: {dataset.id})")
        else:
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            dataset = rag_object.create_dataset(
                name=dataset_name, 
                description="Dataset for user documents",
            )
            print(f"Created new dataset: {dataset.name} (ID: {dataset.id})")
            
    except Exception as e:
        # ê¶Œí•œ ì˜¤ë¥˜ë‚˜ ë‹¤ë¥¸ ì˜¤ë¥˜ ë°œìƒ ì‹œ ìƒˆ dataset ìƒì„±
        if "don't own" in str(e).lower() or "not found" in str(e).lower():
            print(f"Dataset access error: {e}")
            print("Creating new dataset with unique name...")
            dataset = rag_object.create_dataset(name=dataset_name, description="Dataset for user documents")
            print(f"Created new dataset: {dataset.name} (ID: {dataset.id})")
        else:
            print(f"Unexpected error: {e}")
            raise
    
    if not dataset:
        print("Failed to get or create dataset")
        return

    # 1. ë¨¼ì € datasetì— ìˆëŠ” ê¸°ì¡´ ë¬¸ì„œ ëª©ë¡ í™•ì¸
    print(f"\n{'='*60}")
    print("ğŸ“‚ Checking existing documents in dataset...")
    print(f"{'='*60}")
    
    existing_doc_names = set()
    try:
        existing_docs = dataset.list_documents()
        for doc in existing_docs:
            existing_doc_names.add(doc.name)
            print(f"  ğŸ“„ Existing: {doc.name} (ID: {doc.id})")
        
        if existing_docs:
            print(f"\nâœ… Found {len(existing_docs)} existing document(s)")
        else:
            print(f"\nğŸ“­ No existing documents in dataset")
            
    except Exception as e:
        print(f"âš ï¸  Error listing existing documents: {e}")
        existing_doc_names = set()

    # 2. test_pdf í´ë”ì˜ PDF íŒŒì¼ ëª©ë¡ ìƒì„±
    print(f"\n{'='*60}")
    print("ğŸ“ Scanning PDF files in test_pdf folder...")
    print(f"{'='*60}")
    
    pdf_folder = Path('./test_pdf')
    documents_to_upload = []
    skipped_files = []
    
    # test_pdf í´ë”ì˜ ëª¨ë“  PDF íŒŒì¼ ì½ê¸°
    for pdf_file in pdf_folder.glob('*.pdf'):
        file_name = pdf_file.name
        
        # ì¤‘ë³µ ì²´í¬
        if file_name in existing_doc_names:
            print(f"  â­ï¸  Skipped (already exists): {file_name}")
            skipped_files.append(file_name)
            continue
        
        try:
            with open(pdf_file, 'rb') as f:
                documents_to_upload.append({
                    'display_name': file_name,
                    'blob': f.read()
                })
                print(f"  â• Added to upload queue: {file_name}")
        except Exception as e:
            print(f"  âŒ Error reading {file_name}: {e}")
    
    # 3. ì—…ë¡œë“œ ìš”ì•½
    print(f"\n{'='*60}")
    print("ğŸ“Š Upload Summary")
    print(f"{'='*60}")
    print(f"  Total files found: {len(list(pdf_folder.glob('*.pdf')))}")
    print(f"  Files to upload: {len(documents_to_upload)}")
    print(f"  Files skipped (duplicates): {len(skipped_files)}")
    print(f"{'='*60}")
    
    if skipped_files:
        print("\nâ­ï¸  Skipped files:")
        for file_name in skipped_files:
            print(f"    - {file_name}")
    
    # 4. ì—…ë¡œë“œ ì‹¤í–‰
    if not documents_to_upload:
        print("\nğŸ“­ No new documents to upload")
    else:
        print(f"\nâ¬†ï¸  Uploading {len(documents_to_upload)} new document(s)...")
        try:
            result = dataset.upload_documents(documents_to_upload)
            print(f"âœ… Upload result: {result}")
        except Exception as e:
            print(f"âŒ Error uploading documents: {e}")
            return
    
    # 5. ì—…ë¡œë“œëœ ë¬¸ì„œ í™•ì¸ ë° íŒŒì‹±í•  ë¬¸ì„œ ëª©ë¡ ìƒì„±
    print(f"\n{'='*60}")
    print("ğŸ” Checking uploaded documents for parsing...")
    print(f"{'='*60}")
    
    document_infos = []
    try:
        # ì—…ë¡œë“œí•œ íŒŒì¼ ì´ë¦„ ì„¸íŠ¸
        uploaded_file_names = {doc['display_name'] for doc in documents_to_upload}
        
        # ìµœê·¼ ì—…ë¡œë“œëœ ë¬¸ì„œë§Œ íŒŒì‹± ëŒ€ìƒìœ¼ë¡œ ì„ íƒ
        all_docs = dataset.list_documents()
        for doc in all_docs:
            status = get_document_parsing_progress(doc.id)
            if status.get('code') == 0:
                running = status.get('data').get('running')
                progress = status.get('data').get('progress')
                if running == '3' and progress == 1.0:
                    print(f"  âœ… Already parsed: {doc.name} (ID: {doc.id})")
                else:
                    print(f"  âœ… Ready to parse: {doc.name} (ID: {doc.id})")
                    document_infos.append({
                        "doc_id": doc.id,
                        "doc_name": doc.name
                    })
    except Exception as e:
        print(f"âŒ Error listing documents: {e}")
        document_infos = []

    # 6. íŒŒì‹± ì‹¤í–‰
    if document_infos:
        print(f"\nğŸš€ Starting parsing for {len(document_infos)} document(s)...")
        # ìˆœì°¨ì ìœ¼ë¡œ íŒŒì‹± ì‹¤í–‰ (íƒ€ì„ì•„ì›ƒ: 30ë¶„)
        results = asyncio.run(parse_documents_sequentially(document_infos, timeout_per_doc=PARSING_TIMEOUT))
    
        # ê²°ê³¼ ìƒì„¸ ì¶œë ¥
        print("\nğŸ“‹ Detailed Results:")
        for doc_id, result in results.items():
            status_icon = "âœ…" if result["success"] else "âŒ"
            elapsed = result.get("elapsed", 0)
            print(f"  {status_icon} {result['name']}: {'Success' if result['success'] else 'Failed'} ({elapsed}s / {elapsed // 60}min)")
    else:
        print("\nğŸ“­ No documents to parse")

if __name__ == "__main__":
    parse_test()